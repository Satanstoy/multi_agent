# docs/index_docs.py
import os
import glob
import traceback
import argparse
import json
from tqdm import tqdm
import torch

# LangChain ç›¸å…³åº“å¯¼å…¥
from langchain_community.document_loaders import DirectoryLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# --- é»˜è®¤é…ç½® ---
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
ADD_BATCH_SIZE = 100
# --- åµŒå…¥æ¨¡å‹çš„æ‰¹å¤„ç†å¤§å°ï¼Œç”¨äºGPUè®¡ç®—ã€‚å¯æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ä»¥æå‡æ€§èƒ½ã€‚---
EMBED_BATCH_SIZE = 1024

# --- JSON æ–‡ä»¶åŠ è½½å™¨ (æ— å˜åŠ¨) ---
def load_cail_scm_from_json(files_to_process: list) -> list:
    """
    ä»ç»™å®šçš„æ–‡ä»¶åˆ—è¡¨åŠ è½½ CAIL-SCM æ ¼å¼çš„ JSON æ–‡ä»¶,
    å¹¶å°†æ¯ä¸ªæ¡ˆä»¶çš„ A, B, C æ–‡ä¹¦è§£æä¸ºç‹¬ç«‹çš„ LangChain Document å¯¹è±¡ã€‚

    :param files_to_process: éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
    :return: ä¸€ä¸ªåŒ…å«æ‰€æœ‰è§£æå‡ºçš„ Document å¯¹è±¡çš„åˆ—è¡¨ã€‚
    """
    documents = []
    doc_id_counter = 0
    for file_path in files_to_process:
        file_name = os.path.basename(file_path)
        print(f"  - æ­£åœ¨å¤„ç†æ–°æ–‡ä»¶: {file_name}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(tqdm(f, desc=f"  è§£æ {file_name}", unit="è¡Œ")):
                    if not line.strip():
                        continue
                    
                    data = json.loads(line)
                    
                    for key in ['A', 'B', 'C']:
                        if key in data and data[key]:
                            doc = Document(
                                page_content=data[key],
                                metadata={
                                    "source": file_name,
                                    "doc_id": f"case_{doc_id_counter}"
                                }
                            )
                            documents.append(doc)
                            doc_id_counter += 1
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯: è§£æ JSON æ–‡ä»¶ '{file_path}' çš„ç¬¬ {i+1} è¡Œæ—¶å‡ºé”™: {e}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: è¯»å–æˆ–å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}")
            
    return documents


def create_vector_store(source_dir: str, db_dir: str, model_path: str, doc_type: str):
    """
    é€šç”¨å‡½æ•°ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹åŠ è½½æ–‡æ¡£, åˆ†å‰², åˆ›å»ºå‘é‡å­˜å‚¨ã€‚
    ç°åœ¨æ”¯æŒå¢é‡æ›´æ–°ã€‚
    """
    if not os.path.isdir(source_dir):
        print(f"âŒ é”™è¯¯ï¼šæºç›®å½• '{source_dir}' ä¸å­˜åœ¨ã€‚")
        return
    if not os.path.isdir(model_path):
        print(f"âŒ é”™è¯¯ï¼šæŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: '{model_path}'")
        return

    log_file_path = os.path.join(db_dir, 'processed_files.log')
    processed_files = set()
    if os.path.exists(log_file_path):
        with open(log_file_path, 'r', encoding='utf-8') as f:
            processed_files = set(line.strip() for line in f)
    print(f"ğŸ“– å·²å¤„ç†æ–‡ä»¶æ—¥å¿—æ‰¾åˆ° {len(processed_files)} æ¡è®°å½•ã€‚")

    # 1. åŠ è½½æ–‡æ¡£ (åªåŠ è½½æ–°æ–‡ä»¶)
    print(f"\nğŸ“š æ­£åœ¨ä» '{source_dir}' æ£€æŸ¥å¹¶åŠ è½½æ–°çš„ {doc_type} æ–‡æ¡£...")
    
    files_to_process = []
    if doc_type == 'legal':
        all_files = glob.glob(os.path.join(source_dir, '*.docx'))
        files_to_process = [f for f in all_files if os.path.basename(f) not in processed_files]
    elif doc_type == 'case':
        all_files = glob.glob(os.path.join(source_dir, '**', '*.json'), recursive=True)
        files_to_process = [f for f in all_files if os.path.basename(f) not in processed_files]

    if not files_to_process:
        print("âœ… æœªå‘ç°éœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶ã€‚æ•°æ®åº“å·²æ˜¯æœ€æ–°ã€‚")
        return

    print(f"ğŸ“‚ å‘ç° {len(files_to_process)} ä¸ªæ–°æ–‡ä»¶éœ€è¦å¤„ç†: {[os.path.basename(f) for f in files_to_process]}")
    
    documents = []
    try:
        if doc_type == 'legal':
            for file_path in files_to_process:
                 loader = Docx2txtLoader(file_path)
                 documents.extend(loader.load())
        elif doc_type == 'case':
            documents = load_cail_scm_from_json(files_to_process)
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šåŠ è½½æ–°æ–‡æ¡£æ—¶å‡ºé”™: {e}"); traceback.print_exc(); return
    
    if not documents:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªèƒ½ä»æ–°æ–‡ä»¶ä¸­åŠ è½½ä»»ä½•æ–‡æ¡£å†…å®¹ã€‚")
        return
        
    print(f"âœ… æˆåŠŸä»æ–°æ–‡ä»¶ä¸­åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ã€‚")

    # 2. åˆ†å‰²æ–‡æ¡£
    print("\nâœ‚ï¸ æ­£åœ¨å°†æ–°æ–‡æ¡£åˆ†å‰²æˆå—...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, 
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""],
        keep_separator=False
    )
    docs_splitted = text_splitter.split_documents(documents)
    print(f"âœ… å·²å°†æ–°æ–‡æ¡£åˆ†å‰²æˆ {len(docs_splitted)} ä¸ªæ–‡æœ¬å—ã€‚")

    # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nğŸ§  è‡ªåŠ¨æ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡: {device.upper()}")
    # --- ä¿®æ­£ï¼šç§»é™¤ä¸LangChainå†…éƒ¨è°ƒç”¨å†²çªçš„ 'show_progress_bar' å‚æ•° ---
    embeddings = SentenceTransformerEmbeddings(
        model_name=model_path, 
        model_kwargs={'device': device},
        encode_kwargs={'batch_size': EMBED_BATCH_SIZE}
    )

    # 4. æ›´æ–°å‘é‡å­˜å‚¨
    abs_db_dir = os.path.abspath(db_dir)
    print(f"\nğŸ’¾ å‡†å¤‡å‘å‘é‡å­˜å‚¨åº“æ·»åŠ æ–°æ•°æ®: {abs_db_dir}")
    os.makedirs(abs_db_dir, exist_ok=True)
    try:
        vector_store = Chroma(persist_directory=abs_db_dir, embedding_function=embeddings)
        print(f"â³ å¼€å§‹åˆ†æ‰¹æ·»åŠ  {len(docs_splitted)} ä¸ªæ–°æ–‡æœ¬å— (æ‰¹å¤§å°: {ADD_BATCH_SIZE})...")
        for i in tqdm(range(0, len(docs_splitted), ADD_BATCH_SIZE), desc="åµŒå…¥å¹¶å­˜å‚¨", unit="æ‰¹"):
            batch = docs_splitted[i:i + ADD_BATCH_SIZE]
            vector_store.add_documents(documents=batch)
        print("\nâ³ æ­£åœ¨æŒä¹…åŒ–æ•°æ®åº“...")
        vector_store.persist()
        
        print(f"âœï¸ æ­£åœ¨æ›´æ–°å¤„ç†æ—¥å¿—: {log_file_path}")
        with open(log_file_path, 'a', encoding='utf-8') as f:
            for file_path in files_to_process:
                f.write(os.path.basename(file_path) + '\n')

        print(f"ğŸ‰ å‘é‡å­˜å‚¨æ›´æ–°æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šåœ¨åµŒå…¥æˆ–å­˜å‚¨åˆ° Chroma æ—¶å‡ºé”™: {e}"); traceback.print_exc()
    finally:
        vector_store = None; embeddings = None; import gc; gc.collect()

# --- ä¸»ç¨‹åºå…¥å£ (æ— å˜åŠ¨) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸ºæ³•å¾‹æˆ–æ¡ˆä¾‹æ–‡æ¡£åˆ›å»ºå‘é‡æ•°æ®åº“ã€‚")
    parser.add_argument('--type', type=str, choices=['legal', 'case'], required=True, help="è¦ç´¢å¼•çš„æ–‡æ¡£ç±»å‹: 'legal' (æ³•æ¡) æˆ– 'case' (æ¡ˆä¾‹)ã€‚")
    args = parser.parse_args()

    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)

    if args.type == 'legal':
        SOURCE_DIRECTORY = os.path.join(script_dir, "legal")
        PERSIST_DIRECTORY = os.path.join(script_dir, "legal_db")
        print("--- é€‰æ‹©æ¨¡å¼: æ³•æ¡ (legal) ---")
    else: 
        SOURCE_DIRECTORY = os.path.join(script_dir, "case", "CAIL2019-SCM")
        PERSIST_DIRECTORY = os.path.join(script_dir, "case_db")
        print("--- é€‰æ‹©æ¨¡å¼: æ¡ˆä¾‹ (case) ---")
    
    base_dir = os.path.dirname(project_root)
    EMBEDDING_MODEL_PATH = os.path.join(base_dir, "models", "m3e-base")

    print("-" * 60)
    print(f"å‡†å¤‡ä¸º '{args.type}' ç±»å‹æ–‡æ¡£åˆ›å»º/æ›´æ–°å‘é‡æ•°æ®åº“")
    print(f"  - æºæ–‡ä»¶ç›®å½•: {os.path.abspath(SOURCE_DIRECTORY)}")
    print(f"  - ç›®æ ‡æ•°æ®åº“: {os.path.abspath(PERSIST_DIRECTORY)}")
    print(f"  - åµŒå…¥æ¨¡å‹:   {os.path.abspath(EMBEDDING_MODEL_PATH)}")
    print("-" * 60)

    create_vector_store(
        source_dir=SOURCE_DIRECTORY,
        db_dir=PERSIST_DIRECTORY,
        model_path=EMBEDDING_MODEL_PATH,
        doc_type=args.type
    )

    print("-" * 60)
    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
    print("-" * 60)