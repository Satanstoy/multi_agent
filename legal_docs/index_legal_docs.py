# legal_docs/index_legal_docs.py
import os
import glob
import traceback
from tqdm import tqdm
from langchain_community.document_loaders import DirectoryLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

# --- é…ç½® ---
SOURCE_DIRECTORY = "./"  # å‡è®¾ DOCX æ–‡ä»¶åœ¨æ­¤è„šæœ¬æ‰€åœ¨çš„ç›®å½•ä¸‹
PERSIST_DIRECTORY = "legal_db"  # å‘é‡æ•°æ®åº“ç›®å½•ï¼Œç›¸å¯¹äºè„šæœ¬è¿è¡Œä½ç½®

# --- ä¿®æ”¹ï¼šæŒ‡å®šåŒ…å«æ¨¡å‹æ–‡ä»¶çš„æœ¬åœ°ç›®å½•è·¯å¾„ ---
script_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–è„šæœ¬æ‰€åœ¨ç›®å½• (/.../legal_docs)
project_root = os.path.dirname(script_dir) # è·å–ä¸Šä¸€çº§ç›®å½• (/.../multi_agent)
# æ˜ç¡®æŒ‡å‘ multi_agent/models/m3e-base ç›®å½•
EMBEDDING_MODEL_LOCAL_PATH = os.path.join(project_root, "models", "m3e-base")
# --- æœ¬åœ°è·¯å¾„é…ç½®ç»“æŸ ---

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
ADD_BATCH_SIZE = 100
# --- é…ç½®ç»“æŸ ---

# --- ä¾èµ–æé†’ ---
# pip install docx2txt tqdm sentence-transformers
# pip install langchain-community langchain-text-splitters chromadb
# --- ä¾èµ–æé†’ç»“æŸ ---

def create_vector_store():
    """åŠ è½½ DOCX, åˆ†å‰², ä»æœ¬åœ°è·¯å¾„åŠ è½½åµŒå…¥æ¨¡å‹, åˆ›å»ºå‘é‡å­˜å‚¨"""

    # 0. æ£€æŸ¥æºç›®å½•
    if not os.path.isdir(SOURCE_DIRECTORY):
        print(f"âŒ é”™è¯¯ï¼šæºç›®å½• '{SOURCE_DIRECTORY}' ä¸å­˜åœ¨ã€‚")
        return

    # 1. æŸ¥æ‰¾æ–‡ä»¶
    print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ç›®å½• '{os.path.abspath(SOURCE_DIRECTORY)}' ä¸­çš„ .docx æ–‡ä»¶...")
    try:
        docx_files = glob.glob(os.path.join(SOURCE_DIRECTORY, "*.docx"), recursive=False)
        if not docx_files:
            print(f"âš ï¸ è­¦å‘Šï¼šåœ¨ç›®å½• '{os.path.abspath(SOURCE_DIRECTORY)}' ä¸­æœªæ‰¾åˆ°ä»»ä½• .docx æ–‡ä»¶ã€‚")
            return
        print(f"  æ‰¾åˆ° {len(docx_files)} ä¸ª .docx æ–‡ä»¶ã€‚")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šæŸ¥æ‰¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 2. åŠ è½½æ–‡æ¡£
    print(f"\nğŸ“š å¼€å§‹åŠ è½½ {len(docx_files)} ä¸ª DOCX æ–‡æ¡£ (ä½¿ç”¨ Docx2txtLoader)...")
    loader = DirectoryLoader(
        SOURCE_DIRECTORY, glob="*.docx", loader_cls=Docx2txtLoader,
        show_progress=True, use_multithreading=False, recursive=False, silent_errors=True
    )
    try:
        documents = loader.load()
    except ImportError as e:
        if 'docx2txt' in str(e).lower(): print(f"âŒ é”™è¯¯ï¼šç¼ºå°‘ 'docx2txt' åº“ã€‚è¯·è¿è¡Œ 'pip install docx2txt'ã€‚")
        else: print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return
    except Exception as e: print(f"âŒ é”™è¯¯ï¼šåŠ è½½ DOCX æ–‡æ¡£æ—¶å‡ºé”™: {e}"); traceback.print_exc(); return
    if not documents: print(f"âš ï¸ è­¦å‘Šï¼šæœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£å†…å®¹ã€‚"); return
    print(f"\nâœ… æˆåŠŸåŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ã€‚")

    # 3. åˆ†å‰²æ–‡æ¡£
    print("\nâœ‚ï¸ æ­£åœ¨å°†åŠ è½½çš„æ–‡æ¡£å†…å®¹åˆ†å‰²æˆå—...")
    try:
        # åœ¨ç¬¬ 71 è¡Œé™„è¿‘
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP, # <-- æ”¹å› chunk_overlap
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", " ", ""],
            keep_separator=False
        )
        docs_splitted = []
        print("  (è¿›åº¦æ¡åŸºäºå¤„ç†çš„æ–‡æ¡£å¯¹è±¡)")
        for doc in tqdm(documents, desc="åˆ†å‰²æ–‡æ¡£å¯¹è±¡", unit="æ–‡æ¡£"):
            try: chunks = text_splitter.split_documents([doc]); docs_splitted.extend(chunks)
            except Exception as split_err: source_file = doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'); print(f"\nâš ï¸ è­¦å‘Šï¼šåˆ†å‰² '{os.path.basename(source_file)}' æ—¶å‡ºé”™: {split_err}")
    except Exception as e: print(f"âŒ é”™è¯¯ï¼šåˆå§‹åŒ–åˆ†å‰²å™¨æ—¶å‡ºé”™: {e}"); traceback.print_exc(); return
    if not docs_splitted: print("âŒ é”™è¯¯ï¼šæœªèƒ½æˆåŠŸåˆ†å‰²ä»»ä½•æ–‡æ¡£å—ã€‚"); return
    print(f"âœ… å·²å°†æ–‡æ¡£åˆ†å‰²æˆ {len(docs_splitted)} ä¸ªæ–‡æœ¬å—ã€‚")

    # --- 4. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (ä»æœ¬åœ°è·¯å¾„åŠ è½½) ---
    print(f"\nğŸ§  æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL_LOCAL_PATH}")

    # æ£€æŸ¥æŒ‡å®šè·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.isdir(EMBEDDING_MODEL_LOCAL_PATH):
        print(f"âŒ é”™è¯¯ï¼šæŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: '{EMBEDDING_MODEL_LOCAL_PATH}'")
        print(f"   è¯·ç¡®ä¿è¯¥è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”å…¶ä¸­åŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ (config.json, pytorch_model.bin ç­‰)ã€‚")
        return

    try:
        # --- ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„ä½œä¸º model_name ---
        embeddings = SentenceTransformerEmbeddings(
            model_name=EMBEDDING_MODEL_LOCAL_PATH, # <-- ä½¿ç”¨æœ¬åœ°è·¯å¾„
            model_kwargs={'device': 'cpu'}
            # å½“ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾„æ—¶ï¼Œä¸å†éœ€è¦ cache_folder å‚æ•°
        )
        # --- ä¿®æ”¹ç»“æŸ ---
        print("  æµ‹è¯•åµŒå…¥æ¨¡å‹åŠ è½½...")
        _ = embeddings.embed_query("æµ‹è¯•") # è§¦å‘åŠ è½½
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šä»æœ¬åœ°è·¯å¾„ '{EMBEDDING_MODEL_LOCAL_PATH}' åˆå§‹åŒ–åµŒå…¥æ¨¡å‹æ—¶å‡ºé”™ã€‚")
        print(f"   è¯·ç¡®ä¿è¯¥è·¯å¾„ä¸‹åŒ…å«å®Œæ•´ä¸”æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶ä¸” sentence-transformers åº“å·²å®‰è£…ã€‚é”™è¯¯: {e}")
        traceback.print_exc()
        return
    print("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–/åŠ è½½æˆåŠŸã€‚")

    # --- 5. åˆ›å»º/æ›´æ–°å‘é‡å­˜å‚¨ ---
    # (é€»è¾‘ä¸å˜)
    abs_persist_dir = os.path.abspath(PERSIST_DIRECTORY)
    print(f"\nğŸ’¾ å‡†å¤‡åˆ›å»º/æ›´æ–°å‘é‡å­˜å‚¨äº: {abs_persist_dir}")
    os.makedirs(abs_persist_dir, exist_ok=True)
    try:
        print("  åˆå§‹åŒ– Chroma æ•°æ®åº“è¿æ¥...")
        vector_store = Chroma(persist_directory=abs_persist_dir, embedding_function=embeddings)
        print(f"â³ å¼€å§‹åˆ†æ‰¹æ·»åŠ /åµŒå…¥ {len(docs_splitted)} ä¸ªæ–‡æœ¬å—åˆ°æ•°æ®åº“ (æ‰¹å¤§å°: {ADD_BATCH_SIZE})...")
        for i in tqdm(range(0, len(docs_splitted), ADD_BATCH_SIZE), desc="åµŒå…¥å¹¶å­˜å‚¨å—", unit="æ‰¹"):
            batch = docs_splitted[i:i + ADD_BATCH_SIZE]
            try: vector_store.add_documents(documents=batch)
            except Exception as add_err: print(f"\nâš ï¸ è­¦å‘Šï¼šæ·»åŠ æ‰¹æ¬¡ {i//ADD_BATCH_SIZE + 1} åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {add_err}")
        print("\nâ³ æ­£åœ¨æŒä¹…åŒ–æ•°æ®åº“ (ç¡®ä¿æ‰€æœ‰æ›´æ”¹å·²å†™å…¥)...")
        vector_store.persist()
        # æ¸…ç† vector_store å¯¹è±¡å’ŒåµŒå…¥æ¨¡å‹ï¼Œå°è¯•é‡Šæ”¾å†…å­˜
        print("  æ¸…ç†å†…å­˜...")
        vector_store = None
        embeddings = None
        import gc
        gc.collect() # å°è¯•å¼ºåˆ¶åƒåœ¾å›æ”¶
        print(f"ğŸ‰ å‘é‡å­˜å‚¨åˆ›å»º/æ›´æ–°å¹¶æŒä¹…åŒ–æˆåŠŸï¼æ•°æ®åº“ä½äº '{abs_persist_dir}'ã€‚")
    except Exception as e: print(f"âŒ é”™è¯¯ï¼šåœ¨åµŒå…¥æˆ–å­˜å‚¨å—åˆ° Chroma æ—¶å‡ºé”™: {e}"); traceback.print_exc()


if __name__ == "__main__":
    print("-" * 50)
    print("å¼€å§‹åˆ›å»º/æ›´æ–°å‘é‡æ•°æ®åº“ (ä½¿ç”¨ Docx2txtLoader, ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹)...") # æ›´æ–°æç¤º
    print("-" * 50)
    create_vector_store()
    print("-" * 50)
    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
    print("-" * 50)