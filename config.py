# config.py
import os
from typing import List, Dict, Any
from openai import OpenAI # ç¡®ä¿å®‰è£…äº† openai åº“: pip install openai
from copy import deepcopy
import traceback

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¦ç”¨æ¨¡å‹éªŒè¯
os.environ["LITELLM_SKIP_MODEL_VALIDATION"] = "TRUE"

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå®Œå…¨ç¦ç”¨LiteLLMè·¯ç”±
os.environ["LITELLM_DISABLE_ROUTER"] = "TRUE"

# --- LLM Wrapper Class ---
# è¿™ä¸ªç±»å°è£…äº†ä¸ DeepSeek æˆ–å…¼å®¹ OpenAI çš„ API çš„äº¤äº’é€»è¾‘
class DeepSeekLLM:
    def __init__(self, model: str, base_url: str, api_key: str, temperature: float = 0.0):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ã€‚
        :param model: ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚
        :param base_url: API çš„åŸºç¡€ URLã€‚
        :param api_key: API å¯†é’¥ã€‚
        :param temperature: ç”Ÿæˆæ–‡æœ¬çš„æ¸©åº¦å‚æ•° (0 è¡¨ç¤ºæ›´ç¡®å®šæ€§)ã€‚
        """
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.temperature = temperature
        try:
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è¿æ¥åˆ°æœ¬åœ°DeepSeekæœåŠ¡
            self.client = OpenAI(base_url=base_url, api_key=api_key)
            print(f"OpenAI client initialized for model '{model}' at '{base_url}'")
        except Exception as e:
            print(f"Error initializing OpenAI client: {e}")
            self.client = None

    def __call__(self, messages: List[Dict], **kwargs) -> str:
        """ç®€åŒ–ç‰ˆæœ¬çš„æ¶ˆæ¯é‡ç»„"""
        if not self.client:
            return "Error: OpenAI client not initialized."
        
        try:
            print("\n=============== å¼€å§‹è°ƒç”¨LLM ===============")
            print(f"åŸå§‹æ¶ˆæ¯æ•°é‡: {len(messages)}")
            for i, msg in enumerate(messages):
                print(f"  åŸå§‹æ¶ˆæ¯ {i+1}: role={msg.get('role', 'unknown')}, content={msg.get('content', '')[:50]}...")
            
            # æ”¶é›†æ‰€æœ‰è§’è‰²çš„æ¶ˆæ¯å†…å®¹
            all_content = ""
            for msg in messages:
                if msg.get("content") and isinstance(msg.get("content"), str):
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")
                    all_content += f"[{role}]: {content}\n\n"
            
            # åˆ›å»ºå•ä¸ªç”¨æˆ·æ¶ˆæ¯
            simplified_messages = [{"role": "user", "content": all_content.strip() + "\n\n[assistant]:"}]
            
            print(f"Simplifying message structure to a single user message:")
            print(f"  Content: {simplified_messages[0]['content'][:100]}...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=simplified_messages,
                temperature=self.temperature,
            )
            
            content = response.choices[0].message.content
            print(f"è°ƒç”¨ç»“æœ: {content[:100]}...")
            print("=============== LLMè°ƒç”¨å®Œæˆ ===============\n")
            return content.strip() if content else "Error: LLM returned empty content."
        
        except Exception as e:
            print("\n=============== LLMè°ƒç”¨é”™è¯¯ ===============")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯æ¶ˆæ¯: {str(e)}")
            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
            print("=============== é”™è¯¯è¯¦æƒ…ç»“æŸ ===============\n")
            return f"Error during LLM call: {str(e)}"

# --- å®ä¾‹åŒ– LLM ---
# ä½¿ç”¨é€šç”¨æ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šå…·ä½“æ¨¡å‹
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-3.5-turbo") # ä½¿ç”¨ä¸€ä¸ªå¸¸è§çš„æ¨¡å‹åç§°
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://localhost:8000/v1") 
LLM_API_KEY = os.getenv("LLM_API_KEY", "sk-no-key-required")

# åˆ›å»ºå…¨å±€ LLM å®ä¾‹ï¼Œä¾›å…¶ä»–æ¨¡å—å¯¼å…¥ä½¿ç”¨
llm = DeepSeekLLM(
    model=LLM_MODEL,
    temperature=0.1,
    base_url=LLM_BASE_URL,
    api_key=LLM_API_KEY
)

# æ‰“å°åˆå§‹åŒ–ä¿¡æ¯ï¼Œç¡®è®¤é…ç½®åŠ è½½æƒ…å†µ
print("-" * 30)
print(f"LLM é…ç½®åŠ è½½:")
print(f"  æ¨¡å‹ (Model): {LLM_MODEL}")
print(f"  åŸºç¡€ URL (Base URL): {LLM_BASE_URL}")
print(f"  API Key ä½¿ç”¨: {'æ˜¯' if LLM_API_KEY != 'EMPTY' else 'å¦ (æˆ–ä½¿ç”¨é»˜è®¤å ä½ç¬¦)'}")
print("-" * 30)

# åœ¨Agentå®ä¾‹åŒ–æ—¶ï¼Œä¿®æ”¹æ‰§è¡Œæ–¹æ³•
def custom_execute_task(self, task, context=None, **kwargs):
    """ç›´æ¥è°ƒç”¨OpenAIå®¢æˆ·ç«¯è€Œä¸æ˜¯é€šè¿‡LiteLLMï¼Œå¹¶æ‰“å°Agentè¾“å‡º"""
    try:
        # è¿™é‡Œç›´æ¥ä½¿ç”¨DeepSeekLLMå®ä¾‹è€Œä¸æ˜¯Agentçš„llmå±æ€§
        from config import llm

        print(f"\n===== Agent: {self.role} å¼€å§‹æ‰§è¡Œä»»åŠ¡ =====") # æ·»åŠ  Agent è§’è‰²æ‰“å°
        print(f"ä»»åŠ¡æè¿° (ç‰‡æ®µ): {task.description[:100]}...") # æ‰“å°ä»»åŠ¡æè¿°ï¼ˆå¯é€‰ï¼‰
        if context:
            print(f"ä¸Šä¸‹æ–‡ (ç‰‡æ®µ): {str(context)[:100]}...") # æ‰“å°ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

        # æ„å»ºä»»åŠ¡æç¤º
        task_prompt = task.description
        if context:
            # ç¡®ä¿ä¸Šä¸‹æ–‡æ˜¯å­—ç¬¦ä¸²ï¼Œå› ä¸º decision_task çš„è¾“å‡ºæ˜¯å­—ç¬¦ä¸²
            task_prompt = task_prompt.format(context=str(context))

        # åˆ›å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": f"ä½ æ˜¯{self.role}ã€‚{self.goal}"},
            {"role": "user", "content": task_prompt}
        ]

        # ç›´æ¥è°ƒç”¨æˆ‘ä»¬çš„DeepSeekLLMå®ä¾‹
        result = llm(messages) # result å·²ç»æ˜¯æœ€ç»ˆçš„å­—ç¬¦ä¸²è¾“å‡ºäº†

        print(f"\nğŸ’¡ Agent: {self.role} çš„è¾“å‡º:") # æ·»åŠ  Agent è¾“å‡ºæ ‡è®°
        print(result)                              # æ‰“å° Agent çš„å®Œæ•´è¾“å‡º
        print(f"===== Agent: {self.role} ä»»åŠ¡æ‰§è¡Œå®Œæ¯• =====") # æ·»åŠ ç»“æŸæ ‡è®°

        return result # è¿”å›ç»“æœä¾› crewai æµç¨‹ç»§ç»­

    except Exception as e:
        print(f"âŒ Agent: {self.role} æ‰§è¡Œä»»åŠ¡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() # æ‰“å°è¯¦ç»†é”™è¯¯
        return f"Task execution failed for agent {self.role}: {str(e)}"

# æ›¿æ¢Agentçš„execute_taskæ–¹æ³•
from crewai.agent import Agent
Agent.execute_task = custom_execute_task